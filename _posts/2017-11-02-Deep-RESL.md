**MuJoCo and Roboschool**

We are supporting both *MuJoCo* and *OpenAI Roboschool* as physics engines for simulation environments:

+ *MuJoCo*, a physics engine developed at the University of Washington, is a well respected and used simulator in robotics research labs, but requires a paid license (though free student licenses for personal and class work are available).
+ *Roboschool*, based on the open-source *Bullet Physics Engine*, tackles this barrier by removing this license constraint, while supporting the same skeleton definitions as MuJoCo.

Supporting both RoboSchool and MuJoCo enables our platform to reach a wider public.

<iframe width="420" height="315" src="https://storage.googleapis.com/joschu-public/demo-race.mp4" frameborder="1" allowfullscreen></iframe>

**Representing input skeleton and mocap data in Roboschool**

We want to be able to represent both the MOCAP data and rollouts of our generated policy in our simulation environments. Therefore we built a skeleton based on MOCAP data and a parser that can convert the motion capture data into Software, developed at USC, exists for reading in and replaying MOCAP skeletons and data. However, a small parsing script is required to 

Also the definition 

A result of how a loaded skeleton and its real-life movement data look like in Roboschool is shown below:

[![Demo CountPages alpha](https://share.gifyoutube.com/KzB6Gb.gif)](https://www.youtube.com/watch?v=ek1j272iAmc)

**Reinforcement Learning a policy**

In this classical setting an agent in an environment is trains a policy of operation using a reinforcement learning algorithm. In our case we opted for Trusted Region Policy Optimization (TRPO) which has a parallellized implementation in *Baselines*. We trained our modified MOCAP skeleton using TRPO for 10.000 iterations. The result after XXX (1900) iterations is shown below:

<video autoplay="autoplay" loop="loop"a width="768" height="512">
  <source src="/assets/videos/deepresl/TRPO1900.webm" type="video/webm">
  Could not play webm file.
</video>

This policy can then be used in the next step for Vanilla GAIL, i.e. it is an easy replacement for our to-be-imitated real life data.

**Vanilla GAIL**

For this setup, we need to have an available trained policy as an input to the Generative Adversarial Network (GAN) that will serve as an imitation learner. To make things easier we use a policy trained on the same target skeleton with a Reinforcement Learning algorithm (TRPO). This is considered 'Vanilla GAIL' as the imitation is done on a reinforcement learned policy rather than real-life data.

The input (reinforcement learned policy), and output (GAIL policy) of our system is shown below:

[![Demo CountPages alpha](https://share.gifyoutube.com/KzB6Gb.gif)](https://www.youtube.com/watch?v=ek1j272iAmc)

**Data interpolation**

The MOCAP data frames we use are sampled at a different timestep than the timestep which is used for frames in our similators. Accordingly, an up/downsampling step is required.